{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Constants\n",
    "\n",
    "TOT_FILES = 13 #Number of histogram files to merge\n",
    "input_path_start = \"/Users/toddhodges/Desktop/Data/July_5_20/\"\n",
    "output_path_start = \"/Users/toddhodges/Desktop/Data/July_5_20/\"\n",
    "input_file_end = \".hist\"\n",
    "output_file_end = \".hist\"\n",
    "\n",
    "#Flag veriables\n",
    "data_shift = 0      # 0 = No shift, 1 = shift right (positive), -1 = shift left (negative)\n",
    "add_sub = 0    # 0 = No addition or subtraction, 1 = addition and/or subtraction will occur (of summed sets from others)\n",
    "\n",
    "#Load time values\n",
    "data_holder = np.loadtxt(input_path_start + str(1) + input_file_end, delimiter=\",\") #Time index same for all files\n",
    "data_time =  data_holder[:,0]\n",
    "\n",
    "#Merged file arrays\n",
    "no_shift_merged = np.zeros(len(data_time))\n",
    "pos_shift_merged = np.zeros(len(data_time))\n",
    "neg_shift_merged = np.zeros(len(data_time))\n",
    "\n",
    "\n",
    "\n",
    "#Loop through all files and add based on shift flag\n",
    "for n in range(1, (TOT_FILES + 1)):\n",
    "    data_holder = np.loadtxt(input_path_start + str(n) + input_file_end, delimiter=\",\")\n",
    "    data_counts = data_holder[:,1]\n",
    "\n",
    "    if data_shift == 0:                     #This should also have // or add_sub == 1 //\n",
    "        no_shift_merged += data_counts\n",
    "        #pos_shift_merged += data_counts\n",
    "        #neg_shift_merged += data_counts\n",
    "\n",
    "    if data_shift == 1 or add_sub == 1:\n",
    "        #print (1)\n",
    "        temp_data = np.zeros(len(data_time))\n",
    "        for i in range(len(data_counts) - (n - 1)):\n",
    "            #temp_data = np.zeros(len(data_time))\n",
    "            temp_data[i + (n - 1)] = data_counts[i]\n",
    "\n",
    "        #print(n)\n",
    "        pos_shift_merged += temp_data\n",
    "        #pos_shift_merged = np.sum(pos_shift_merged, temp_data)\n",
    "        #plt.plot(data_time, temp_data)\n",
    "    #plt.show()\n",
    "\n",
    "    if data_shift == -1 or add_sub == 1:\n",
    "        #print(-1)\n",
    "        temp_data = np.zeros(len(data_time))\n",
    "        for i in range(len(data_counts) - (n - 1)):\n",
    "            #temp_data = np.zeros(len(data_time))\n",
    "            temp_data[i] = data_counts[i + (n - 1)]\n",
    "\n",
    "        neg_shift_merged += temp_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create file with times and counts after summing\n",
    "total_combined = np.zeros((len(data_time), 2))\n",
    "total_combined[:,0] = data_time[:]\n",
    "total_combined[:,1] = no_shift_merged[:]    #merged_data_counts[:]\n",
    "#print(total_combined)\n",
    "np.savetxt(output_path_start + \"combined_unaltered\" + output_file_end, total_combined, delimiter=',')\n",
    "\n",
    "\n",
    "#Plot things\n",
    "if add_sub == 1:\n",
    "\n",
    "    no_pos = np.subtract(no_shift_merged, pos_shift_merged)\n",
    "    no_neg = np.subtract(no_shift_merged, neg_shift_merged)\n",
    "    pos_neg = np.subtract(pos_shift_merged, neg_shift_merged)\n",
    "\n",
    "    plt.plot(data_time, no_pos)\n",
    "    plt.title(\"Zero Shift - Positive Shift\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(data_time, no_neg)\n",
    "    plt.title(\"Zero Shift - Negative Shift\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(data_time, pos_neg)\n",
    "    plt.title(\"Positive Shift - Negative Shift\")\n",
    "    plt.show()\n",
    "\n",
    "elif data_shift == 0:\n",
    "    plt.plot(data_time, no_shift_merged)\n",
    "    plt.title(\"Zero Shift\")\n",
    "    plt.show()\n",
    "\n",
    "elif data_shift == 1:\n",
    "    plt.plot(data_time, pos_shift_merged)\n",
    "    plt.title(\"Positive Shift\")\n",
    "    plt.show()\n",
    "\n",
    "elif data_shift == -1:\n",
    "    plt.plot(data_time, neg_shift_merged)\n",
    "    plt.title(\"Negative Shift\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import array as ar\n",
    "import math\n",
    "#import glob, time, os\n",
    "#from scipy import signal\n",
    "#import scipy\n",
    "\n",
    "# Constants\n",
    "clockTimePS = 5000  # Clock cycle time in ps\n",
    "TOTAL_FILES = 1 #Total number of files\n",
    "combined_file = \"combined_unaltered.hist\"\n",
    "combined_out = \"combined_CT.hist\"\n",
    "input_file_path = \"C:\\\\Users\\\\Daniel\\\\Desktop\\\\Codes\\\\DNL Code\\\\\"\n",
    "output_file_path = \"C:\\\\Users\\\\Daniel\\\\Desktop\\\\Codes\\\\DNL Code\\\\\"\n",
    "count_rate = 4.48 #MHz\n",
    "\n",
    "for z in range(1, (TOTAL_FILES + 1)):\n",
    "    in_file = str(z) + \".hist\"\n",
    "    out_file = str(z) + \"_CT.hist\"\n",
    "\n",
    "\n",
    "    # Data Arrays\n",
    "    #uncorData = np.loadtxt('cor_1h_long_short_20h.hist', delimiter=',')  # assumed to be 2x1 [time shift, coincidence count]\n",
    "    ## corData = np.loadtxt('1.hist', delimiter=',')  # assumed to be 2x1 [time shift, coincidence count]\n",
    "    #uncorData = np.loadtxt('simFile.hist', delimiter=',')  # This uses simulated data from Signal Simulation.py\n",
    "    #uncorData = np.loadtxt('uncor_1h_long_short_24h.hist', delimiter=',')  # assumed to be 2x1 [time shift, coincidence count]\n",
    "    #uncorData = np.loadtxt('polar_12hour.hist', delimiter=',')  # assumed to be 2x1 [time shift, coincidence count]. DEBUGGING\n",
    "    uncorData = np.loadtxt(input_file_path + in_file, delimiter=',')  # assumed to be 2x1 [time shift, coincidence count]. DEBUGGING\n",
    "    #uncorData = np.loadtxt(input_file_path + combined_file, delimiter=',') #MULTIPLE FILES\n",
    "    ## corTime = corData[:, 0]  # 1D array of time shift values for correlated data\n",
    "    ## corCount = corData[:, 1]  # 1D array of total coincidence counts for correlated data\n",
    "    uncorTime = uncorData[:, 0]  # 1D array of total time shift values for the uncorrelated data\n",
    "    uncorCount = uncorData[:, 1]  # 1D array of total coincidence counts for uncorrelated data\n",
    "\n",
    "    # Find the total number of clock cycles\n",
    "    # Commented out for the data slicing test below\n",
    "    #uncorClockCycles = int(len(uncorCount) / clockTimePS)  # Total clock cycles. Each time increment in the hist is 1ps\n",
    "\n",
    "    plt.plot(uncorTime, uncorCount)     #DEBUGGING\n",
    "    #print(np.average(uncorCount[394899:405501]))       #DEBUGGING\n",
    "    print(np.average(uncorCount))       #DEBUGGING\n",
    "    plt.show()                          #DEBUGGING\n",
    "\n",
    "    #print(\"Shape:\", uncorData.shape)   #DEBUGGING\n",
    "\n",
    "\n",
    "    # ====Start Section: Second rebin to 25ps blocks====\n",
    "\n",
    "    newBinSize = 25  # (CAN CHANGE)\n",
    "    reBinUncorCount = np.arange(len(uncorCount) / newBinSize) #(FROM 0 TO THE LENGTH OF UNCORCOUNT / NEWBINSIZE)\n",
    "    newClockCycle = int(clockTimePS / newBinSize)  # Number of elements per clock cycle after rebinning\n",
    "\n",
    "    for i in range(0, len(reBinUncorCount)):\n",
    "        newUncorCount = 0\n",
    "        for k in range(0, newBinSize):\n",
    "            newUncorCount = newUncorCount + uncorCount[k + i * newBinSize]\n",
    "        reBinUncorCount[i] = newUncorCount\n",
    "\n",
    "    # This plot is with 25ps rebinning\n",
    "    #n = np.array(range(-16000, 16000))  # DEBUGGING\n",
    "    #plt.plot(n, reBinUncorCount)  # DEBUGGING\n",
    "    #plt.xlabel('Time Bin Shifts (25ps/shift)')\n",
    "    #plt.ylabel('Coincidence Counts')\n",
    "    #plt.show()  # DEBUGGING\n",
    "\n",
    "    # ====End Section: Second rebin to 25ps blocks====\n",
    "\n",
    "\n",
    "    # =========Start Section: Data Trimming==========\n",
    "\n",
    "    # Cut out portion of reBinUncorCount and reBinCorCount\n",
    "    remWindow = 5000  # Distance from midpoint in both direction. Total of 10000 elements #Cuts out the middle of the data set(CAN CHANGE)\n",
    "    midPoint = int(len(reBinUncorCount)/2)\n",
    "    trimmedUncorCount = np.arange(len(reBinUncorCount)-(remWindow*2))\n",
    "\n",
    "    for i in range(0, (midPoint - remWindow)):\n",
    "        trimmedUncorCount[i] = reBinUncorCount[i]\n",
    "\n",
    "    for j in range((midPoint + remWindow), len(reBinUncorCount)):\n",
    "        trimmedUncorCount[j - 2 * remWindow] = reBinUncorCount[j]\n",
    "\n",
    "\n",
    "    reBinUncorCount = trimmedUncorCount\n",
    "    uncorClockCycles = int(len(reBinUncorCount) / newClockCycle)  # Needed since we have cut the total number of clock cycles\n",
    "\n",
    "    # This plot is with 25ps rebinning and the middle 10000 elements removed\n",
    "    #n = np.array(range(-11000, 11000))  # DEBUGGING\n",
    "    #plt.plot(n, reBinUncorCount)  # DEBUGGING\n",
    "    #plt.xlabel('Time Bin Shifts (25ps/shift) w/ Data Trimming')\n",
    "    #plt.ylabel('Coincidence Counts')\n",
    "    #plt.show()  # DEBUGGING\n",
    "\n",
    "    # =========End Section: Data Trimming==========\n",
    "\n",
    "\n",
    "    # =========Start Section: DNL Correction==========\n",
    "\n",
    "    # Find the counts per sub clock bin if evenly distributed (over all clock cycles)\n",
    "    # If purely random, no time shift should change total coincidence counts\n",
    "    #totalCount = sum(uncorCount)  # sum of individual clock cycle sums\n",
    "    totalCount = sum(reBinUncorCount)  # Added for the above debugging/testing\n",
    "    totalBins = newClockCycle * uncorClockCycles  # newClockCycle reflects rebinning\n",
    "    evenCountsBin = totalCount / totalBins  # This value should be <N_bin>\n",
    "\n",
    "    # Determine the individual average count per sub clock bin across all clock cycles. These values are (b_i BAR)\n",
    "    indBinAvg = np.arange(newClockCycle)*1.0  # Multiplying by 1.0 forces the array elements to keep float data type\n",
    "    for i in range(0, newClockCycle):  # Look at each sub clock element one at a time. REMOVED subElements for debugging\n",
    "        binSum = 0\n",
    "        for k in range(0, uncorClockCycles):  # Sum the value for the same sub clock element over each clock cycle\n",
    "            binSum = binSum + reBinUncorCount[(k * newClockCycle) + i]\n",
    "        # print('i ='+ str(i) +',k = '+str(k) + ' : ' + str(uncorCount[(k*clockTimePS) + i])) #debugging\n",
    "        indBinAvg[i] = float(binSum) / float(uncorClockCycles)  # b_i BAR. Should be able to remove float recasts\n",
    "        # print(binSum) #debugging\n",
    "\n",
    "    # Find the correction coefficient for each bin. This value is (D_i)\n",
    "    DNLCo = np.arange(len(indBinAvg))*1.0\n",
    "    for i in range(0, len(indBinAvg)):\n",
    "    #    DNLCo[i] = 1 #USED FOR SIGNAL GENERATOR RUNS\n",
    "        DNLCo[i] = indBinAvg[i]/evenCountsBin #TEMP REMOVE FOR SIGNAL GENERATOR TEST\n",
    "\n",
    "\n",
    "    # Correct counts in correlated data to remove DNL contribution. I'll need a double loop\n",
    "    correctedCounts = np.arange(len(reBinUncorCount))*1.0\n",
    "    for j in range(0, uncorClockCycles):\n",
    "        for i in range(0, newClockCycle):\n",
    "            correctedCounts[i + j * newClockCycle] = reBinUncorCount[i + j * newClockCycle]/DNLCo[i]\n",
    "\n",
    "    # =========End Section: DNL Correction==========\n",
    "\n",
    "    ##### Add some code here to plot with first rebin, data trim, and DNL correction ###############################\n",
    "    #n = np.array(range(-11000, 11000))  # DEBUGGING\n",
    "    #plt.plot(n, correctedCounts)  # DEBUGGING\n",
    "    #plt.xlabel('Time Bin Shifts (25ps/shift) w/ Data Trimming and DNL Correction')\n",
    "    #plt.ylabel('Coincidence Counts')\n",
    "    #plt.show()  # DEBUGGING\n",
    "    ################################################################################################################\n",
    "\n",
    "    # ==== Start Section: Second rebin to 250ps blocks ====\n",
    "\n",
    "    newBinSize = 10 # Already rebinned into 25ps bins. So, 10*25 = 250ps bins\n",
    "    reBinCorrectedCounts = np.arange(math.floor(len(correctedCounts) / newBinSize)) #Should use np.zeros()\n",
    "    #print(len(correctedCounts))         #DEBUGGING\n",
    "    #print(len(reBinCorrectedCounts))    #DEBUGGING\n",
    "\n",
    "    for i in range(0, len(reBinCorrectedCounts)):\n",
    "        newCorrectedCount = 0\n",
    "        for k in range(0, newBinSize):\n",
    "            newCorrectedCount = newCorrectedCount + correctedCounts[k + i * newBinSize]\n",
    "        reBinCorrectedCounts[i] = newCorrectedCount\n",
    "\n",
    "    correctedCounts = reBinCorrectedCounts.copy()\n",
    "\n",
    "    # ==== End Section: Second rebin to 250ps blocks ====\n",
    "    #print(len(correctedCounts))         #DEBUGGING\n",
    "\n",
    "\n",
    "    # Plot corrected counts (DNL correction applied)\n",
    "    if len(correctedCounts) % 2 == 0:           #Length of used corrected counts is even (see above)\n",
    "        n = np.array(range((int(-1*len(correctedCounts)/2)), int((len(correctedCounts)/2))))  # -400ns to +400ns rebinned\n",
    "    else:                                       #Length of used corrected counts is odd (see above)\n",
    "        n = np.array(range((int(-1 * len(correctedCounts) / 2)), int((len(correctedCounts) / 2)+1)))  # -400ns to +400ns rebinned\n",
    "    plt.plot(n, correctedCounts)\n",
    "    #plt.plot(n, trimmedUncorCount) #DEBUGGING\n",
    "    plt.xlabel('Time Bin Shifts (250ps/shift)')\n",
    "    plt.ylabel('Corrected Coincidence Counts')\n",
    "    #plt.title(\"Hour \" + str(z) + \" (Count Rate: \" + str(count_rate) + \"MHz)\")\n",
    "    #plt.title(\"Combined File\" + \" (Count Rate: \" + str(count_rate) + \"MHz)\")\n",
    "    #print(np.average(correctedCounts)) #DEBUGGING\n",
    "    plt.show()\n",
    "\n",
    "    #Optional plot of rebinned correlated counts before correction\n",
    "    #plt.plot(n, reBinCorCount)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ==== Start: Write new 2d array and out to file ====\n",
    "\n",
    "    finalData = np.zeros((int(len(n)), 2))\n",
    "    #print(finalData.shape) #DEBUGGING\n",
    "    #print(len(n)) #DEBUGGING\n",
    "\n",
    "    for i in range(0, len(n)):\n",
    "        finalData[(i, 0)] = n[i]\n",
    "        #print(finalData[(i, 0)]) #DEBUGGING\n",
    "\n",
    "    for j in range(0, len(n)):\n",
    "        finalData[(j, 1)] = correctedCounts[j]\n",
    "\n",
    "    #print(finalData) #DEBUGGING\n",
    "\n",
    "    np.savetxt(output_file_path + out_file, finalData, delimiter=',')\n",
    "    #np.savetxt(output_file_path + combined_out, finalData, delimiter=',')\n",
    "    # Change file location/name for each use. Also, can write a .txt to check formatting and data visually\n",
    "\n",
    "    # ==== End: Write new 2d array and out to file ====\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # =======DEBUGGING/TESTING========\n",
    "\n",
    "    #x = np.array(range(len(DNLCo)))\n",
    "    #plt.plot(x, DNLCo)\n",
    "    #plt.show()\n",
    "\n",
    "    #for m in range(0,10):\n",
    "    #    print(DNLCo[m])\n",
    "\n",
    "    #for x in range(0,int(len(uncorCount)/1000)):\n",
    "    #  if uncorCount[x] != 0:\n",
    "    #    print(str(x) + ' : ' + str(uncorCount[x]))\n",
    "\n",
    "\n",
    "    #print(x)\n",
    "    #print(uncorCount[0])\n",
    "\n",
    "    # DEBUGGING/TESTING\n",
    "    #testArray = np.arange(len(indBinAvg))*1.0\n",
    "    #for x in range(0, len(indBinAvg)):\n",
    "    #    testArray[x] = indBinAvg[x]/DNLCo[x]\n",
    "\n",
    "    #plt.plot(n, testArray)\n",
    "    #plt.show()\n",
    "\n",
    "    #for x in range(0, len(DNLCo)):\n",
    "    #    print(DNLCo[x])\n",
    "\n",
    "    # Testing\n",
    "    # print(len(n))\n",
    "    # print(correctedCounts[10000])\n",
    "    # print(len(reBinCorCount))\n",
    "    # print(DNLCo[0])\n",
    "    # print(indBinAvg[0])\n",
    "    # print(uncorData[0,0])\n",
    "    # print(uncorData[0,1])\n",
    "    # print(uncorCount[0])\n",
    "    # print(evenCountsBin)\n",
    "    # print((indBinAvg[0])/(evenCountsBin))\n",
    "    # print(len(indBinAvg))\n",
    "    # print(len(n))\n",
    "    # print(newClockCycle)\n",
    "    # print(indBinAvg[0])\n",
    "    # print(indBinAvg[199])\n",
    "    # print(indBinAvg[2])\n",
    "    # print(totalCount)\n",
    "    # print(uncorClockCycles)\n",
    "    # print(totalBins)\n",
    "    # print(np.where(uncorCount==0,1,0))\n",
    "    # print(uncorCount)\n",
    "    # print(len(uncorCount))\n",
    "    # print(uncorTime[0])\n",
    "    # print(uncorTime[1])\n",
    "\n",
    "\n",
    "    # Semantic errors exist\n",
    "\n",
    "    # TO DO LIST\n",
    "    # Use a data set from one of our runs to see if the same pattern exists. Check Snowpea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
